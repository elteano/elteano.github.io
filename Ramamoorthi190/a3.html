<html>
	<head>
		<link href='style.css' rel='stylesheet'>
		<link href='a3.css' rel='stylesheet'>
	</head>
	<body>
		Presented by Thomas Tucker, A10755095.
		<h1>Assignment 3</h1>
		<p>
			For this assignment, I will be implementing the Image Analogies algorithm
			as discussed in the paper available <a
			href='http://www.mrl.nyu.edu/publications/image-analogies/analogies-fullres.pdf'>here</a>.
		</p>
		<h4>Usage</h4>
		<p>
			The provided 
		</p>
		<h2>Overview</h2>
		<p>
			The process of Image Analogies has four major steps, as described in the
			paper. First, Gaussian Pyramids are created by blurring and then
			subsampling an image, blurring and then subsampling the resulting image,
			and repeating this process until one has an image which is too small to
			be blurred and subsampled. In the paper, a level of the pyramid with a
			low number represents a level with high coarseness - that is, the image
			has been blurred and subsampled more times than a level with a higher
			number. However, I represent them in the opposite manner, as this was the
			manner I felt was most naturally implemented. An example of a Gaussian
			Pyramid flattened into a single image is displayed below.
		</p>
		<img src='a3/gpflower.bmp' style='width:240; height: 120;'></img><br/>
		<p>
			The Gaussian Pyramid allows the image to be analyzed at differing levels
			of coarseness; because there are fewer pixels in a higher level of the
			pyramid, a broader scope of the image's features may be inspected
			using the same operations as would be used to analyze fine details at a
			lower level of the pyramid.
		</p>
		<p>
			The next step is to calculate the feature vectors for each pixel. These
			are used for searching and comparing which pixel is ideal for each
			location as the new image is constructed. While it would make initial
			sense for these to be the RGB values for each pixel, and it is suggested
			as such in the paper, further inquiry shows that the ideal feature vector
			for each pixel is actually just the single luminance value for the pixel.
			In the paper, it is referred to as the Y value in the YIQ scale. This
			allows scenes which are diverse in color to be compared intuitively, as
			the colors are effectively normalized to the luminance scale. The colors
			are then reconstructed from the B image in a sort of luminance transfer;
			an example is displayed below, in which the luminance from the first
			image is transferred to the second to produce the third.
		</p>
		<img class='fimg' src='a3/reflect_corner.bmp'></img> <img class='fimg'
		src='a3/flower.bmp'></img> <img class='fimg' src='a3/lflower.bmp'></img>
		<p>
			An early result from the implementation can be seen below. The first two
			images were used for training (A and A'), the third image as the base (B)
			image, and the final image is the resulting B' image. Key points to note
			are that while the top of the resulting image lacks any particular
			texture, the bottom two thirds are textured with various paint-style
			strokes, although with visible edge artifacts and not particularly
			sensible placement. In particular, one can notice that the imperfect
			implementation favors grabbing Lucian Freud&rsquo;s ear and other large
			chunks of his face rather than mimicking single brush strokes. However,
			the created image is certainly more abstract than the base image.
		</p>
		<img class='refl' src="a3/reflection_a.bmp"/> <img class='refl'
		src="a3/reflection_a'.bmp"/> <img class='shore' src='a3/shore.bmp'/> <img
		class='shore' src='a3/intershore.bmp'/>
	</body>
</html>

