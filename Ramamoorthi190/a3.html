<html>
	<head>
		<link href='style.css' rel='stylesheet'>
		<link href='a3.css' rel='stylesheet'>
		<title>Ramamoorthi 190 Assignment 3</title>
	</head>
	<body>
		Presented by Thomas Tucker, A10755095.
		<h1>Assignment 3</h1>
		<p>
			For this assignment, I will be implementing the Image Analogies algorithm
			as discussed in the paper available <a
			href='http://www.mrl.nyu.edu/publications/image-analogies/analogies-fullres.pdf'>here</a>.
		</p>
		<h4>Execution</h4>
		<p>
			The executable and required library is available to <a href='a3/A3.zip'>download
			here</a>.<br/>
			In order to create an image analogy, the suggested execution model is
			<span class='mono'>A3 -k &lt;weight&gt; -epsilon &lt;error&gt; -analogy
			&lt;A&gt; &lt;A'&gt; &lt; &lt;base&gt; &gt; &lt;output&gt;</span>. For
			all of the images on this page, the k and epsilon values are listed.
		</p>
		<p>
			The k value is the weight between coherence and approximate matching. The
			paper discusses different weights they used; if none is specified, the
			program defaults to k=5.
		</p>
		<p>
			The epsilon value is the amount of error allowed to the Approximate
			Nearest Neighbor search. Increasing this value above its default of zero
			will significantly speed up execution (and possibly make the resulting
			image look better).
		</p>
		<h5>Other Options</h5>
		<p>
			There are also a few debug options available, listed here.<br/>
			<span class='mono'>-reduce</span><br/>
			This will run a single pass of the reduce function which is used to
			create the Gaussian Pyramids. This first applies a gaussian blur, then
			does a simple subsample to reduce the image&rsquo;s dimensions by
			half.<br/>
			<span class='mono'>-max_levels</span><br/>
			This will limit the maximum number of levels the Gaussian Pyramids are
			permitted. This may be desirable if all source images are very large,
			although the higher levels of the Gaussian Pyramids require radically
			less computation than the lower, and so saved time will be marginal.<br/>
			<span class='mono'>-gp</span><br/>
			This calculates the Gaussian Pyramid for the source image and outputs the
			flattened pyramid.<br/>
			<span class='mono'>-rgp &lt;out_file&gt;</span><br/>
			This outputs the pyramid calculated during the creation of the image
			analogy to the given output file.<br/>
			<span class='mono'>-lumi &lt;base_file&gt;</span><br/>
			This does a luminance transfer from the base file to the standard input
			file.<br/>
		</p>
		<h2>Overview</h2>
		<p>
			The process of Image Analogies has four major steps, as described in the
			paper. First, Gaussian Pyramids are created by blurring and then
			subsampling an image, blurring and then subsampling the resulting image,
			and repeating this process until one has an image which is too small to
			be blurred and subsampled. In the paper, a level of the pyramid with a
			low number represents a level with high coarseness - that is, the image
			has been blurred and subsampled more times than a level with a higher
			number. However, I represent them in the opposite manner, as this was the
			manner I felt was most naturally implemented. An example of a Gaussian
			Pyramid flattened into a single image is displayed below.
		</p>
		<img src='a3/gpflower.bmp' style='width:240; height: 120;'></img><br/>
		<p>
			The Gaussian Pyramid allows the image to be analyzed at differing levels
			of coarseness; because there are fewer pixels in a higher level of the
			pyramid, a broader scope of the image's features may be inspected
			using the same operations as would be used to analyze fine details at a
			lower level of the pyramid.
		</p>
		<p>
			The next step is to calculate the feature vectors for each pixel. These
			are used for searching and comparing which pixel is ideal for each
			location as the new image is constructed. While it would make initial
			sense for these to be the RGB values for each pixel, and it is suggested
			as such in the paper, further inquiry shows that the ideal feature vector
			for each pixel is actually just the single luminance value for the pixel.
			In the paper, it is referred to as the Y value in the YIQ scale. This
			allows scenes which are diverse in color to be compared intuitively, as
			the colors are effectively normalized to the luminance scale. The colors
			are then reconstructed from the B image in a sort of luminance transfer;
			an example is displayed below, in which the luminance from the first
			image is transferred to the second to produce the third.
		</p>
		<img class='fimg' src='a3/reflect_corner.bmp'></img> <img class='fimg'
		src='a3/flower.bmp'></img> <img class='fimg' src='a3/lflower.bmp'></img>
		<p>
			A result from the implementation can be seen below. The first two images
			were used for training (A and A'), the third image as the base (B) image,
			and the final image is the resulting B' image. Key points to note are
			that while the top of the resulting image lacks any particular texture,
			the bottom two thirds are textured with various paint-style strokes,
			although with visible edge artifacts and not particularly sensible
			placement. In particular, one can notice that the imperfect
			implementation favors grabbing Lucian Freud&rsquo;s ear and other large
			chunks of his face rather than mimicking single brush strokes. However,
			the created image is certainly more abstract than the base image. This
			was created with the hardcoded default values of k=5 and epsilon=0.0.
		</p>
		<img class='refl' src="a3/reflection_a.bmp"/> <img class='refl'
		src="a3/reflection_a'.bmp"/> <img class='shore' src='a3/shore.bmp'/> <img
		class='shore' src='a3/intershore.bmp'/>
		<p>
			Note, however, that the writers of the paper recognize that there must be
			some input tweaking in order to get good results. Using the same A and A'
			images as above, and the B image below, the B' image below was created
			using k=1, epsilon=5.0. Note that more of the &lsquo;artistic
			effect&rsquo; was captured here, although there are some artifacts
			created from a faulty YIQ mapping.
		</p>
		<a href='a3/shed.bmp'><img src='a3/shed.bmp'/></a> <a
		href='a3/paintshed.bmp'><img src='a3/paintshed.bmp'/></a><br/>
		Note that these images have been scaled to fit the page layout. Clicking on
		the image will link to the full size version.
		<p>
			While far from pretty, the gaussian pyramid generated alongside this
			image is displayed below. Like the images above, it has been scaled to
			fit the page.
		</p>
		<a href='a3/paintshed_rgp.bmp'><img src='a3/paintshed_rgp.bmp'/></a>
		<p>
			After fixing the YIQ bug, the image and gaussian pyramid below were
			generated using the same values as above (k=1, epsilon=5.0).
		</p>
		<a href='a3/new_paintshed.bmp'><img src='a3/new_paintshed.bmp'/></a><br/>
		<a href='a3/new_paintshed_rgp.bmp'><img src='a3/new_paintshed_rgp.bmp'/></a>
		<p>
			These results are looking pretty good; however, it should be noted that
			no displacement is occuring here. The color at each pixel is the
			luminance from the selected A' pixel matched with the I and Q values from
			the source B pixel at the destination location; thus, despite the strokes
			applied to the roof of the shed being curved, one can notice that there
			remains a distinct straight edge to the shed. This is extracted from the
			shed for clarity below.
		</p>
		<img src='a3/badshedroof.bmp' style='width: 153px; height: 122px;'/>
	</body>
</html>

