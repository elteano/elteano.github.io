<html>

  <head>
    <title>Thomas Tucker's Website</title>
    <link rel='stylesheet' type='text/css' href='style/style.css'> </head>

  <body>
    <h1>Thomas Tucker</h1>
    <h3>Degrees</h3> All degrees are from the University of California, San Diego (UCSD).
    <ul>
      <li>MS in Computer Science (awarded 2017)</li>
      <li>BS in Computer Science (awarded 2016)</li>
      <li>Minor in Mathematics (awarded 2016)</li>
    </ul>
    <h2>Graphics</h2>
    <h3> CSE 167 at UCSD: </h3>
    <p> Final project, a collaboration with my peer Marco Mendez. I personally created
      the bloom effects and the water effects; my partner was responsible for the
      multi-textured model and (probably) the particle system. This was coded in
      C++ using OpenGL. </p>
    <p> The image on the left has bloom active, the image on the right does not. The
      bloom is most visible in the particle system emerging from the cannon and the
      clouds in the skybox. </p>
    <p> <img class='half' src='images/blooming.png' alt='Reflective water with bloom filter.'
      /> <img class='half' src='images/nobloom.png' alt='Reflective water without bloom filter.'
      /> </p>
    <p> The bloom effects are implemented using a Gaussian blur over the highlights of
      the scene. First, the items are rendered as normal; they are then filtered
      for the highlights, which are extracted into a separate framebuffer. The highlights
      are then flipped back and forth between two framebuffers, each time blurring
      either on the horizontal or vertical axes. The result is then added back to
      the base image to create the bloom effect. </p>
    <p> The water reflection is implemented by rendering the image twice, only rendering
      what is above the water. The first render uses the default camera; the second
      render uses the camera's position reflected beneath the water, so its new depth
      below the water is the same as its previous height above the water, and the
      angle is now tilted up to the same degree as it was tilted down. The second
      image is then transformed using a du/dv map to create the wave effect for things
      reflected from above the water (models, skybox, and the like). The two images
      are then painted onto the water quad, using the camera angle to determine how
      they are mixed in order to replicate the <a href='https://en.wikipedia.org/wiki/Fresnel_equations'>Fresnel effect</a>.
      </p>
    <h3> CSE 190 at UCSD </h3>
    <p> There were three assignments for this course. For more information on individual
      assignments, follow the <a href='Ramamoorthi190'>rabbit hole</a>. The final
      project was a combination of image processing and an artificial intelligence
      which attempted to replicate a given art style on an image. The user provides
      two sample images which establish the art style (one is unstyled, one is styled),
      as well as an image to be transformed, and the software attempts to replicate
      the given art style on the final image. This is based on the <a href='https://www.mrl.nyu.edu/publications/image-analogies/analogies-fullres.pdf'><i>Image
      Analogies</i></a> paper by Hertzman, and others. The input images were provided
      <a href='https://www.mrl.nyu.edu/projects/image-analogies/'>here</a>. This
      was coded in C++. </p>
    <p> Input images: <br/> <img class='half' src='Ramamoorthi190/a3/reflection_a.bmp'
        alt='Unstyled input image' /> <img class='half' src="Ramamoorthi190/a3/reflection_a'.bmp"
        alt='Styled input image' /> </p>
    <p> Target and output images: <br/> <img class='half' src='Ramamoorthi190/a3/shed.bmp'
      /> <img class='half' src='Ramamoorthi190/a3/new_paintshed.bmp' /> </p>
    <h2>
    Other Projects </h2>
    <h3>Chopsticks</h3>
    <p> A client-server implementation of the game <a href='https://en.wikipedia.org/wiki/Chopsticks_(game)'>Chopsticks</a>.
      Source code is available on <a href='https://github.com/elteano/Chopsticks'>GitHub</a>.
      It is written in an Object-Oriented fashion using D, and built with dub. Clients
      and servers may connect through IPv4 sockets or through Unix sockets, if available.
      </p>
    <h3>RNNs</h3>
    <p> I've recently been working on creating a character-level recursive neural network
      which is trained on <i>The Count of Monte Cristo</i> by Alexandre Dumas (p&egrave;re).
      It was originally based off <a href='https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/'>a
      tutorial by Jason Brownlee</a>, and has since grown into a little project on
      its own. The GitHub repository may be perused <a href='https://github.com/elteano/lstm-nonsense'>here</a></p>
    <p>All of the characters are reduced to lower case for the network, so it makes
      no attempt at making correct capitalization. Below is some sample output; its
      grammar is way off, but it otherwise gets spelling mostly correct. It tends
      to generate conversations between Monte Cristo and someone else (quite frequently,
      Monte Cristo will be talking to himself). <pre>
"you know you will be here."

"you will prove you?"

"yes; what do you be to see the count?"

"yes," said monte cristo, "what do you know?"

"you will not have completed me."

"well," said monte cristo; "i will know what you will be with the
carriage."

"and you will not know what you know they were only the
countâ€™s house;"

"yes; i have come to my country."
</pre> This was generated after roughly a day of training on a two-stacked layer
      of LSTM units, with 256 units in each layer. The output layer uses a softmax
      activation function, for which a temperature of 0.3 is provided during generation.</p>
  </body>

</html>